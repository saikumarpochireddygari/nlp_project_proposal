{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2TWfHUtTUoa",
        "outputId": "c4d3271e-040c-48ee-9472-b9af635ee725"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        }
      ],
      "source": [
        "# Code Block 1\n",
        "\n",
        "# Importing Libraries required for NLP\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "# Importing Libraries needed for Tensorflow processing\n",
        "import tensorflow as tf   # version 1.13.2\n",
        "import numpy as np\n",
        "import tflearn            # version 0.3.2\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8NJuQ79J7-e"
      },
      "source": [
        "### Block 1 code description\n",
        "In the above code block we are importing the tools that are necessary to process text data and make a model to learn from that\n",
        "text data in order develop contextual learning. We will be needing the tools/packages namely NLTK, Tensorflow, Numpy, Random, TFlearn, json to achieve our objective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h56jRlSXTeBF"
      },
      "outputs": [],
      "source": [
        "#!pip install tflearn==0.3.2\n",
        "#!pip install tensorflow==1.13.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y0-SW9HyTUoc"
      },
      "outputs": [],
      "source": [
        "# Code block 2 \n",
        "# importing our intent file, which will be used for training the model.\n",
        "with open(\"intents.json\") as json_data: \n",
        "    intents = json.load(json_data)      # Loading data from intents.json file to variable intents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arU0yytmo5HL",
        "outputId": "6e69d276-003b-4dbb-cc2f-0205d41dabd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'tag': 'greeting',\n",
              "  'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'],\n",
              "  'responses': ['Hello, thanks for visiting',\n",
              "   'Good to see you again',\n",
              "   'Hi there, how can I help?'],\n",
              "  'context_set': ''},\n",
              " {'tag': 'goodbye',\n",
              "  'patterns': ['Bye', 'See you later', 'Goodbye'],\n",
              "  'responses': ['See you later, thanks for visiting',\n",
              "   'Have a nice day',\n",
              "   'Bye! Come back again soon.']},\n",
              " {'tag': 'thanks',\n",
              "  'patterns': ['Thanks', 'Thank you', \"That's helpful\"],\n",
              "  'responses': ['Happy to help!', 'Any time!', 'My pleasure']}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "intents['intents'][0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAHu6Ux9Od8B"
      },
      "source": [
        "### Block 2 Code Description\n",
        "In this code block we are reading the data from the file using json tool, Json is an easy light weight key value storage file system.\n",
        "By using this system the file size decreases drastically and query to a json file will be lightning fast when we have lots of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5leZVhw4TUod"
      },
      "outputs": [],
      "source": [
        "# Code block 3 \n",
        "# Empty lists for appending the data after processing NLP\n",
        "words=[]\n",
        "documents = []\n",
        "classes = []\n",
        "\n",
        "# This list will be used for ignoring all unwanted punctuation marks.\n",
        "ignore = [\"?\"]\n",
        "\n",
        "# Starting a loop through each intent in intents[\"patterns\"]\n",
        "for intent in intents[\"intents\"]: # looping through each intent\n",
        "    for pattern in intent[\"patterns\"]: # picking through each pattern from intent\n",
        "        \n",
        "        # tokenizing each and every word in the sentence by using word tokenizer and storing in w\n",
        "        # tokenizing the each pattern_word using tokenizer\n",
        "        w = nltk.word_tokenize(pattern) \n",
        "        #print(w)\n",
        "        \n",
        "        # Adding tokenized pattern words to words empty list that we defined above\n",
        "        words.extend(w) \n",
        "        #print(words)\n",
        "        \n",
        "        # Adding words to documents with tag given in intents file as a tuple format\n",
        "        documents.append((w, intent[\"tag\"]))\n",
        "        #print(documents)\n",
        "        \n",
        "        # Adding only tag to our classes list\n",
        "        if intent[\"tag\"] not in classes:      \n",
        "            classes.append(intent[\"tag\"])  #If tag is not present in classes[] then it will append into it.\n",
        "            #print(classes)\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWQUpr8xrM29"
      },
      "source": [
        "#### Block 3 code explanation\n",
        "In this code block we are generating three lists namely words which contains tokens of the patterns, classes which contains the tag of the particular patterns, and document which containes the pattern and class. For this operation we are using for loop to iterate through the datafile and generate the tokens, pick the classes of each pattern.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3ym0mh1TUod",
        "outputId": "ddbc101d-70df-4af3-948f-e1cc616b0e53",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "88 Documents \n",
            "\n",
            "33 Classes \n",
            "\n",
            "97 Stemmed Words \n"
          ]
        }
      ],
      "source": [
        "# Code Block 4\n",
        "\n",
        "#Performing Stemming by using stemmer.stem() on lowercase of each word \n",
        "#Running loop in words[] and ignoring punctuation marks present in ignore[]\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]  \n",
        "words = sorted(list(set(words)))  #Removing Duplicates in words[]\n",
        "\n",
        "#Removing Duplicate Classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "#Printing length of lists we formed\n",
        "print(len(documents),\"Documents \\n\")\n",
        "print(len(classes),\"Classes \\n\")\n",
        "print(len(words), \"Stemmed Words \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD7zQXWw4XSY",
        "outputId": "dc42e109-8059-4614-b6e7-d55c8a336bb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([(['Hi'], 'greeting'),\n",
              "  (['How', 'are', 'you'], 'greeting'),\n",
              "  (['Is', 'anyone', 'there', '?'], 'greeting')],\n",
              " [\"'s\", 'a', 'act', 'an', 'and'])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0:3] , words[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-drLAt1ctp3K"
      },
      "source": [
        "### Block code 4 explanation\n",
        "In this code block we are deriving the stem words and storing them in words variable and removing the duplicates of the stemmed words. For this task we are utlising nltk library's stemmer package and set data structure. Finally we are knowing the statistics of the documents, classess, and words. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9YVf5eu3TUoe"
      },
      "outputs": [],
      "source": [
        "#Block code 5 Explanation\n",
        "\n",
        "#Creating Training Data which will be furthur used for training\n",
        "training = []\n",
        "output = []\n",
        "\n",
        "#Creating empty array for output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "#Creating Training set and bag of words for each sentence\n",
        "for doc in documents:\n",
        "    bag = [] #Initialising empty bag of words\n",
        "\n",
        "    pattern_words = doc[0] #Storing list of tokenized words for the documents[] tp pattern_words\n",
        "    #print(pattern_words)\n",
        "    \n",
        "    #Again Stemming each word from pattern_words\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]  \n",
        "    #print(pattern_words)\n",
        "    \n",
        "    #Creating bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "        \n",
        "    #It will give output 1 for curent tag and 0 for all other tags\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] =1 \n",
        "    training.append([bag, output_row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eP3oEI12R4i"
      },
      "source": [
        "### Block code 5 Explanation\n",
        "In this code block we are generating data which is used to train the \n",
        "chatbot so that it can learn the patterns. For generating the training data we are utilising our documents variable and iterating it using a for loop, Also for every word in words variable we are performing stemming and checking if the word happens to be in our actual words from document to assign polarity to the particular tag using if else logic. Finally we are generating an array of 1's and 0's and appending it to the training variable that we defined earlier.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V9YN718TUoe",
        "outputId": "f26984a5-b2a2-4b1f-93da-2c7cac163ab0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "# Code Block 6\n",
        "random.shuffle(training) #Suffling the data or features\n",
        "training = np.array(training) #Converting training data into numpy array\n",
        "\n",
        "#Creating Training Lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrb_AAQN_tKy"
      },
      "source": [
        "### Block code 6 explanation\n",
        "In this code block we are shuffling the training data and converting the data to an array with the help of numpy tool that we have imported earler. For any ML model we need two sets of data one is training data and another is label data so that the model can learn the pattern, For this we are breaking the shuffled training data into two parts using subsetting feature of an list data structure. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWy5311wTUof",
        "outputId": "d4d4bc35-b72e-478c-947e-301d833f714c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Step: 10999  | total loss: \u001b[1m\u001b[32m0.46801\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 1000 | loss: 0.46801 - acc: 0.9811 -- iter: 80/88\n",
            "Training Step: 11000  | total loss: \u001b[1m\u001b[32m0.42148\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 1000 | loss: 0.42148 - acc: 0.9829 -- iter: 88/88\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "# Code Block 7 \n",
        "tf.reset_default_graph() #Reset Underlying Graph data\n",
        "\n",
        "#Building our own Neural Network\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])]) #adding an input layer to our neuralnetwork\n",
        "net = tflearn.fully_connected(net, 10) #adding fully connected layer with 10 hidden neurons\n",
        "net = tflearn.fully_connected(net, 10) #adding fully connected layer with 10 hidden neurons\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation=\"softmax\") #adding a fully connected layer with activation function as softmax\n",
        "net = tflearn.regression(net) #adding regression (linear or logistic) to the provided input.\n",
        "\n",
        "#Defining Model and setting up tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir=\"tflearn_logs\")\n",
        "\n",
        "#Now we have setup model, now we need to train that model by fitting data into it by model.fit()\n",
        "#n_epoch is the number of times that model will se our data during training\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)  # training the model using fit method with epoch as 1000 and batch size as 8\n",
        "model.save(\"model.tflearn\") #Saving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLCSjfxTC9ZK"
      },
      "source": [
        "### Block code 7 explaination\n",
        "With the aid of the tflearn package that we previously loaded, we are creating our own deep neural network in this code block. This network consists of three fully connected layers: an input layer, a regression layer, and a regression layer with customized parameters for two of the levels. For the output propagation based on a mathematical function, there is an activation function called softmax. Finally, in order to train on the training data and reduce loss (misinterpretations/error), we are employing this specially built feedforward neural network. We use the network.fit() method for the training, and then we save the model so that we can use it in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "k84Y0u7pTUof"
      },
      "outputs": [],
      "source": [
        "# Code Block 8 \n",
        "\n",
        "#Importing pickle module\n",
        "import pickle\n",
        "\n",
        "#Dumping training data by using dump() and writing it into training_data in binary mode\n",
        "pickle.dump({\"words\":words, \"classes\":classes, \"train_x\":train_x, \"train_y\":train_y}, open(\"training_data\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUij039aFV4m"
      },
      "source": [
        "### Block code 8 explanation\n",
        "In this code block we are saving all our variables as binarys using pickle package that we have imported earlier. The advatage of saving them is for re-usibility and progress saving. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3YoozXRqTUog"
      },
      "outputs": [],
      "source": [
        "# code block 9\n",
        "#Restoring all data structure\n",
        "data = pickle.load(open(\"training_data\",\"rb\")) #using pickle package to load the training data \n",
        "# subsetting all the required data from the data variable\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWI-nDcHS2xZ"
      },
      "source": [
        "### Block code 9 explanation\n",
        "\n",
        "In this code block we are loading all our saved binaries for re-usage "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RdrN_pJKTUog"
      },
      "outputs": [],
      "source": [
        "# Code block 10\n",
        "with open(\"intents.json\") as json_data:\n",
        "    intents = json.load(json_data)  #Loading our json_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8DLPlA4TfL6"
      },
      "source": [
        "### Block code 10 explanation\n",
        "In this code block we are reading data the intents datafile with the help of json package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BMkKFoAITUog"
      },
      "outputs": [],
      "source": [
        "# Code block 11\n",
        "\n",
        "# Loading the saved model\n",
        "model.load(\"./model.tflearn\") #Loading training model which we saved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpjWsCQTVPBL"
      },
      "source": [
        "## Block code 11 explanation\n",
        "In this block of code we are loading the model back to the kernel for re-usability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yHC02sDKTUog"
      },
      "outputs": [],
      "source": [
        "#Code block 12\n",
        "\n",
        "#Cleaning User Input\n",
        "def clean_up_sentence(sentence):\n",
        "    \n",
        "    # Tokenizing the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence) #Again tokenizing the sentence\n",
        "    \n",
        "    #Stemming each word from the user's input\n",
        "    sentence_words= [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "\n",
        "    return sentence_words\n",
        "\n",
        "#Returning bag of words array: 0 or 1 or each word in the bag that exists in as we have declared in above lines\n",
        "def bow(sentence, words, show_details=False):\n",
        "    \n",
        "    #Tokenizing the user input\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    \n",
        "    #Generating bag of words from the sentence that user entered\n",
        "    bag = [0]*len(words) #intialising bag vector with 0's\n",
        "    for s in sentence_words: #iterating through the sentence words\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: #checking if word is the same as the word from user's sentence\n",
        "                bag[i] = 1  #if matched assigning value of 1\n",
        "                if show_details:\n",
        "                    print(\"Found in bag: %s\"% w)\n",
        "    return(np.array(bag)) #cnverting bag to array and returning it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQWWYR0xY2Jv"
      },
      "source": [
        "## Block code 12 explanation\n",
        "In this code block we are defining two utility functions for processing the sentence and converting the sentence to tokens using stemming and tokenization technique. Also, we are converting the senetence into an machine readable form by using bag of words technique which returns an array of 1 and 0 for any given sentence. Later these can be fed into the chatbot model for answering our queries/questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wz4UxqJNTUoh"
      },
      "outputs": [],
      "source": [
        "# Code block 13.\n",
        "\n",
        "#Adding some context to the conversation for better results.\n",
        "\n",
        "context = {} #Create a dictionary to hold user's context.\n",
        "\n",
        "ERROR_THRESHOLD = 0.25\n",
        "def classify(sentence):\n",
        "    \n",
        "    #Generating probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    \n",
        "    #Filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    \n",
        "    #Sorting by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    \n",
        "    # return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "def query(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    \n",
        "    #If we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        \n",
        "        #Loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                \n",
        "                #Find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    \n",
        "                    #Set context for this intent if necessary\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "                    # check if this intent is contextual and applies to this user's conversation\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: print ('tag:', i['tag'])\n",
        "                        \n",
        "                        #A random response from the intent\n",
        "                        return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0) #sending out the response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4bsfwnFg_ht"
      },
      "source": [
        "# Block code 13 Explanation \n",
        "In this code block we are adding the contextual meaning to the model using fucntions. We will be using two fucntions, One does the job of classifying the input sentence into the tags and returns the sorted list of words by probability with the help of the trained model, Second fucntion returns the query result by identifying the tag from the results and filters the context using bunch of if else statements and gives us the response of the query. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no3PFHGlTUoh",
        "outputId": "634ef148-515c-4c5f-ab89-68fe9eba5108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decryption is the process of taking encoded or encrypted text or other data and converting it back into text that you or the computer can read and understand. This term could be used to describe a method of unencrypting the data manually or unencrypting the data using the proper codes or keys.\n"
          ]
        }
      ],
      "source": [
        "query(\"what is data\") #testing the bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDcejp1C1NlB",
        "outputId": "7ce880da-399d-4d7c-f868-a224434de158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi there, how can I help?\n"
          ]
        }
      ],
      "source": [
        "query(\"Hi\") #testing the bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7nuzVLE1U1a",
        "outputId": "3cc74d99-e423-4bfb-8e3e-a8bb874ab716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VPN stands for Virtual Private Network. It is used to create a safe and encrypted connection. When you use a VPN, the data from the client is sent to a point in the VPN where it is encrypted and then sent through the internet to another point. At this point, the data is decrypted and sent to the server. When the server sends a response, the response is sent to a point in the VPN where it is encrypted and this encrypted data is sent to another point in the VPN where it is decrypted. And finally, the decrypted data is sent to the client. The whole point of using a VPN is to ensure encrypted data transfer.\n"
          ]
        }
      ],
      "source": [
        "query(\"I want to learn about vpn\") #testing the bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlQCsl-b1gJI",
        "outputId": "ad6cefc3-7613-4fd6-a3f7-4797e30ff522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grey hat hackers are an amalgamation of a white hat and black hat hacker. They look for system vulnerabilities without the owner�s permission. If they find any vulnerabilities, they report it to the owner. Unlike Black hat hackers, they do not exploit the vulnerabilities found.\n"
          ]
        }
      ],
      "source": [
        "query(\"hat means hackers grey ??\") #testing the bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyiyKzhI1gN9",
        "outputId": "04649869-e615-437b-c0e4-eff21495abc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good to see you again\n"
          ]
        }
      ],
      "source": [
        "query(\"how is ur day going on\") #testing the bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0v-jZlF1gQ3",
        "outputId": "9684061f-a961-4d3a-f9dc-cabb960495a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceroute is a tool that shows the path of a packet. It lists all the points (mainly routers) that the packet passes through. This is used mostly when the packet is not reaching its destination. Traceroute is used to check where the connection stops or breaks to identify the point of failure.\n"
          ]
        }
      ],
      "source": [
        "query(\"grey and vpn\") #testing the bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPA0-e1VWL6h",
        "outputId": "3c7ea201-21b5-43cb-be13-85912839ea33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'intents': [{'tag': 'greeting',\n",
              "   'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'],\n",
              "   'responses': ['Hello, thanks for visiting',\n",
              "    'Good to see you again',\n",
              "    'Hi there, how can I help?'],\n",
              "   'context_set': ''},\n",
              "  {'tag': 'goodbye',\n",
              "   'patterns': ['Bye', 'See you later', 'Goodbye'],\n",
              "   'responses': ['See you later, thanks for visiting',\n",
              "    'Have a nice day',\n",
              "    'Bye! Come back again soon.']},\n",
              "  {'tag': 'thanks',\n",
              "   'patterns': ['Thanks', 'Thank you', \"That's helpful\"],\n",
              "   'responses': ['Happy to help!', 'Any time!', 'My pleasure']},\n",
              "  {'tag': 'hours',\n",
              "   'patterns': ['What hours are you open?',\n",
              "    'What are your hours?',\n",
              "    'When are you open?',\n",
              "    'When is the time to contact ?',\n",
              "    'At what time do you provide services ?'],\n",
              "   'responses': [\"We're open every day from 9AM to 9PM\",\n",
              "    'Our working hours are 9AM to 9PM every day']},\n",
              "  {'tag': 'Cryptography',\n",
              "   'patterns': ['What is mean by cryptography?', 'What is Cryptography?'],\n",
              "   'responses': ['Cryptography is the practice and study of techniques for securing information and communication mainly to protect the data from third parties that the data is not intended for.']},\n",
              "  {'tag': 'Firewall',\n",
              "   'patterns': [' What is a Firewall and why is it used?',\n",
              "    'what is firewall?',\n",
              "    'what firewall does?'],\n",
              "   'responses': ['A Firewall is a network security system set on the boundaries of the system/network that monitors and controls network traffic. Firewalls are mainly used to protect the system/network from viruses, worms, malware, etc. Firewalls can also be to prevent remote access and content filtering.']},\n",
              "  {'tag': 'Penetration Testing',\n",
              "   'patterns': ['What is Penetration Testing?',\n",
              "    'explain Penetration Testing?',\n",
              "    'What is  in cyber security?'],\n",
              "   'responses': ['Penetration Testing is the process of finding vulnerabilities on the target. In this case, the organization would have set up all the security measures they could think of and would want to test if there is any other way that their system/network can be hacked.']},\n",
              "  {'tag': 'Vulnerability Assessment',\n",
              "   'patterns': ['What is Vulnerability Assessment?',\n",
              "    'explain Vulnerability Assessment?',\n",
              "    'What is VA in cyber security?'],\n",
              "   'responses': ['Vulnerability Assessment is the process of finding flaws on the target. Here, the organization knows that their system/network has flaws or weaknesses and want to find these flaws and prioritize the flaws for fixing.']},\n",
              "  {'tag': 'traceroute',\n",
              "   'patterns': ['What is traceroute?', 'explain traceroute?'],\n",
              "   'responses': ['Traceroute is a tool that shows the path of a packet. It lists all the points (mainly routers) that the packet passes through. This is used mostly when the packet is not reaching its destination. Traceroute is used to check where the connection stops or breaks to identify the point of failure.']},\n",
              "  {'tag': 'VPN',\n",
              "   'patterns': ['What is a VPN?', 'explain vpn?', 'why vpn is used?'],\n",
              "   'responses': ['VPN stands for Virtual Private Network. It is used to create a safe and encrypted connection. When you use a VPN, the data from the client is sent to a point in the VPN where it is encrypted and then sent through the internet to another point. At this point, the data is decrypted and sent to the server. When the server sends a response, the response is sent to a point in the VPN where it is encrypted and this encrypted data is sent to another point in the VPN where it is decrypted. And finally, the decrypted data is sent to the client. The whole point of using a VPN is to ensure encrypted data transfer.']},\n",
              "  {'tag': 'black hat hackers',\n",
              "   'patterns': ['what are black hat hackers?', 'black hat hackers?'],\n",
              "   'responses': ['Black hat hackers are known for having vast knowledge about breaking into computer networks. They can write malware which can be used to gain access to these systems. This type of hackers misuse their skills to steal information or use the hacked system for malicious purpose.']},\n",
              "  {'tag': 'White hat hackers',\n",
              "   'patterns': ['what are White hat hackers?', 'ethical hackers?'],\n",
              "   'responses': ['White hat hackers use their powers for good deeds and so they are also called Ethical Hackers. These are mostly hired by companies as a security specialist that attempts to find and fix vulnerabilities and security holes in the systems. They use their skills to help make the security better. ']},\n",
              "  {'tag': 'Grey hat hackers',\n",
              "   'patterns': ['what are Grey hat hackers ?', 'Grey hat hackers?'],\n",
              "   'responses': ['Grey hat hackers are an amalgamation of a white hat and black hat hacker. They look for system vulnerabilities without the owner�s permission. If they find any vulnerabilities, they report it to the owner. Unlike Black hat hackers, they do not exploit the vulnerabilities found.']},\n",
              "  {'tag': 'ARP',\n",
              "   'patterns': ['What is an ARP and how does it work?',\n",
              "    'what is ARP?',\n",
              "    'arp in cyber security?'],\n",
              "   'responses': ['Address Resolution Protocol (ARP)is a protocol for mapping an Internet Protocol address (IP address) to a physical machine address that is recognized in the local network. When an incoming packet destined for a host machine on a particular local area network arrives at a gateway, the gateway asks the ARP program to find a physical host or MAC address that matches the IP address.']},\n",
              "  {'tag': 'Botnet',\n",
              "   'patterns': ['What is a Botnet?', 'Bonet?'],\n",
              "   'responses': ['A Botnet is a number of devices connected to the internet where each device has one or more bots running on it. The bots on the devices and malicious scripts used to hack a victim. Botnets can be used to steal data, send spams and execute a DDOS attack.']},\n",
              "  {'tag': 'Secure Sockets Layers',\n",
              "   'patterns': ['ssl?', 'explain ssl?', 'What is ssl?'],\n",
              "   'responses': [\"SSL is meant to verify the sender's identity but it doesn�t search for anything more than that. SSL can help you track the person you are talking to but that can also be tricked at times.\"]},\n",
              "  {'tag': 'Transport Layer Security',\n",
              "   'patterns': ['tls?', 'explain tls?', 'What is tls?'],\n",
              "   'responses': ['TLS is also an identification tool just like SSL, but it offers better security features. It provides additional protection to the data and hence SSL and TLS are often used together for better protection.']},\n",
              "  {'tag': 'Cognitive Cybersecurity',\n",
              "   'patterns': ['What is Cognitive Cybersecurity?',\n",
              "    'explain Cognitive Cybersecurity?',\n",
              "    'Cognitive Cybersecurity?'],\n",
              "   'responses': ['Cognitive Cybersecurity is an application of AI technologies patterned on human thought processes to detect threats and protect physical and digital systems. Self-learning security systems use data mining, pattern recognition, and natural language processing to simulate the human brain, albeit in a high-powered computer model.']},\n",
              "  {'tag': 'Encryption different from Hashing',\n",
              "   'patterns': ['How is Encryption different from Hashing?',\n",
              "    'Encryption different from Hashing?'],\n",
              "   'responses': ['Both Encryption and Hashing are used to convert readable data into an unreadable format. The difference is that the encrypted data can be converted back to original data by the process of decryption but the hashed data cannot be converted back to original data.']},\n",
              "  {'tag': 'encryption',\n",
              "   'patterns': ['What is encryption?', 'encryption?', 'explain encryption?'],\n",
              "   'responses': ['Encryption is a way of scrambling data so that only authorized parties can understand the information. In technical terms, it is the process of converting plaintext to ciphertext. In simpler terms, encryption takes readable data and alters it so that it appears random. Encryption requires the use of an encryption key: a set of mathematical values that both the sender and the recipient of an encrypted message know.']},\n",
              "  {'tag': 'Decryption',\n",
              "   'patterns': ['What is Decryption?', 'Decryption?', 'explain Decryption?'],\n",
              "   'responses': ['Decryption is the process of taking encoded or encrypted text or other data and converting it back into text that you or the computer can read and understand. This term could be used to describe a method of unencrypting the data manually or unencrypting the data using the proper codes or keys.']},\n",
              "  {'tag': 'Hashing',\n",
              "   'patterns': ['What is Hashing?', 'Hashing?', 'explain Hashing?'],\n",
              "   'responses': ['Hashing is an algorithm performed on data such as a file or message to produce a number called a hash. The hash is used to verify that data is not modified, tampered with, or corrupted. In other words, you can verify the data has maintained integrity.']},\n",
              "  {'tag': 'cybersecurity',\n",
              "   'patterns': ['what is cybersecurity?', 'cybersecurity?'],\n",
              "   'responses': ['Cybersecurity refers to the protection of hardware, software, and data from attackers. The primary purpose of cyber security is to protect against cyberattacks like accessing, changing, or destroying sensitive information.']},\n",
              "  {'tag': 'Forward Secrecy',\n",
              "   'patterns': ['What is forward secrecy?', 'Forward Secrecy?'],\n",
              "   'responses': ['Forward Secrecy is a system that uses ephemeral session keys to do the actual encryption of TLS data so that even if the server�s private key were to be compromised, an attacker could not use it to decrypt captured data that had been sent to that server in the past.']},\n",
              "  {'tag': 'boot sector virus',\n",
              "   'patterns': ['what is boot sector virus?',\n",
              "    'how boot sector virus effects?'],\n",
              "   'responses': ['A boot sector virus is a type of virus that infects the boot sector of floppy disks or the Master Boot Record (MBR) of hard disks (some infect the boot sector of the hard disk instead of the MBR). ... While boot sector viruses infect at a BIOS level, they use DOS commands to spread to other floppy disks.']},\n",
              "  {'tag': 'direct action virus',\n",
              "   'patterns': ['what is direct action virus?',\n",
              "    'how direct action virus effects?'],\n",
              "   'responses': ['A direct action virus is a type of file infector virus that works by attaching itself to an .exe or .com file when installed or executed. Once this occurs, the virus can spread to other existing files and can render them inaccessible.']},\n",
              "  {'tag': 'resident virus',\n",
              "   'patterns': ['what is resident virus?', 'how resident virus effects?'],\n",
              "   'responses': ['A resident virus is a computer virus that stores itself within memory, allowing it to infect other files even when the originally infected program is no longer running.']},\n",
              "  {'tag': 'multipartite virus',\n",
              "   'patterns': ['what is multipartite virus?',\n",
              "    'how multipartite virus effects?'],\n",
              "   'responses': [\"A multipartite virus is a computer virus that's able to attack both the boot sector and executable files of an infected computer. If you're familiar with cyber threats, you probably know that most computer viruses either attack the boot sector or executable files.\"]},\n",
              "  {'tag': 'spacefiller virus',\n",
              "   'patterns': ['what is spacefiller virus?',\n",
              "    'how spacefiller virus effects?'],\n",
              "   'responses': ['Alternatively referred to as a cavity virus, a spacefiller virus is a rare type of computer virus that attempts to install itself by filling in empty sections of a file. By only using empty sections of a file, the virus can infect a file without the size of the file changing, making it more difficult to detect.']},\n",
              "  {'tag': ' file infector virus',\n",
              "   'patterns': ['Do you provide home delivery?',\n",
              "    'Do you deliver the food?',\n",
              "    'What are the home delivery options?'],\n",
              "   'responses': ['A file infector virus attaches itself to executable programs, such as word processors, spreadsheet applications, and computer games. When the virus has infected a program, it propagates to infect other programs on the system, as well as other systems that use a shared infected program. ']},\n",
              "  {'tag': 'computer virus',\n",
              "   'patterns': ['what is computer virus?',\n",
              "    'What do you mean by computer virus?',\n",
              "    'Define Computer Virus'],\n",
              "   'responses': ['A computer virus, much like a flu virus, is designed to spread from host to host and has the ability to replicate itself. Similarly, in the same way that flu viruses cannot reproduce without a host cell, computer viruses cannot reproduce and spread without programming such as a file or document.']},\n",
              "  {'tag': 'Features of a Cybersecurity',\n",
              "   'patterns': ['Important Features of a Cybersecurity?',\n",
              "    'Why we need CyberSecurity ?',\n",
              "    'Whta are features of CyberSecurity?'],\n",
              "   'responses': ['1.Good Analytics, 2.Coverage of your biggest external threats, 3.A defense against internal threats, 4.Compliance, 5.Manage risk across your entire ecosystem, 6.Threat prevention, detection, and response, 7.Continuous monitoring']},\n",
              "  {'tag': 'Benefits of cybersecurity',\n",
              "   'patterns': ['What are the Benefits of cybersecurity?',\n",
              "    'uses of cyber security?'],\n",
              "   'responses': ['1.Business protection against malware, ransomware, phishing and social engineering, 2.Protection for data and networks, 3.Prevention of unauthorized users, 4.Improves recovery time after a breach, 5.Protection for end-users']}]}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "intents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM_0P8HS1FBB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
